Metadata-Version: 2.1
Name: tritoncl
Version: 0.1
Summary: 使用 Triton 实现的 BLAS 库
Home-page: https://github.com/csrddbb/tritoncl
Author: lidongsheng
Author-email: lidsh25@sysu.edu.cn
License: UNKNOWN
Description: 
        # Triton-BLAS
        - [English](README.md)
        - [简体中文](README_CN.md)
        
        Triton-BLAS is a high-performance library implementing all BLAS (Basic Linear Algebra Subprograms) functions and some AI operations using the Triton deep learning compiler. This library is designed for scientific computing and AI workloads, leveraging the power of modern GPUs for efficient computations.
        
        ## Features
        - **BLAS Level 1**: Vector operations (AXPY, DOT, NRM2, etc.)
        - **BLAS Level 2**: Matrix-vector operations (GEMV, TRSV, etc.)
        - **BLAS Level 3**: Matrix-matrix operations (GEMM, SYRK, etc.)
        - **AI Operations**: Common operations in AI such as 2D Convolution, Self-Attention
        
        ## Installation
        
        ### Prerequisites
        Make sure you have the following dependencies installed:
        - Python 3.8+
        - Triton 2.0+ (`pip install triton`)
        - CUDA-enabled GPU with compatible drivers
        
        ### Install via setup.py
        
        To install Triton-BLAS, clone the repository and run:
        
        ```bash
        git clone https://github.com/your-repo/triton-blas.git
        cd triton-blas
        pip install .
        ```
        
        Alternatively, you can install dependencies from `requirements.txt`:
        
        ```bash
        pip install -r requirements.txt
        ```
        
        ## Usage
        
        Once installed, you can import and use the library as follows:
        
        ```python
        import triton_blas.blas_ops.level1.axpy as axpy
        import triton_blas.blas_ops.level3.gemm as gemm
        
        # Example: AXPY operation
        x = [1.0, 2.0, 3.0]
        y = [4.0, 5.0, 6.0]
        alpha = 0.5
        result = axpy.axpy(alpha, x, y)
        print(result)
        
        # Example: GEMM operation (Matrix Multiplication)
        A = [[1, 2], [3, 4]]
        B = [[5, 6], [7, 8]]
        C = gemm.gemm(A, B)
        print(C)
        ```
        
        ### AI Operations
        
        ```python
        import triton_blas.ai_ops.conv2d as conv2d
        
        # Example: 2D Convolution
        input_tensor = ...
        filter_tensor = ...
        output_tensor = conv2d.conv2d(input_tensor, filter_tensor)
        ```
        
        ## Testing
        
        Unit tests are provided for every implemented function. To run the tests, use the following command:
        
        ```bash
        pytest tests/
        ```
        
        ## Benchmarks
        
        Performance benchmarks are available in the `benchmarks/` directory. For example, to benchmark the GEMM implementation:
        
        ```bash
        python benchmarks/benchmark_gemm.py
        ```
        
        ## Contributing
        
        Contributions are welcome! If you wish to contribute, please follow these steps:
        1. Fork the repository
        2. Create a feature branch (`git checkout -b feature-name`)
        3. Commit your changes (`git commit -am 'Add new feature'`)
        4. Push to the branch (`git push origin feature-name`)
        5. Create a pull request
        
        Please make sure your code is well-documented and includes relevant tests.
        
        ## License
        
        This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
        
        
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.6
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
